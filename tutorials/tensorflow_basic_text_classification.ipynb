{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification: Classify images of clothing\n",
    "\n",
    "Tutorial URL: https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "Valid as of: 2023.05.02\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 10s 0us/step\n",
      "Dataset directory contents: ['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']\n",
      "Training subdirectory contents: ['labeledBow.feat', 'neg', 'pos', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n",
      "Sample file contents:\n",
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Review: b'There are so many stupid moments in \\'Tower of Death\\'/\\'Game of Death 2\\' that you really wonder if it\\'s a spoof. At times, it felt like I was watching a sequel to Kung Pow rather than a Bruce Lee film.<br /><br />To be honest, this film has bugger all to do with \\'Game of Death\\'. If anything, it\\'s more a sequel/remake of \\'Enter the Dragon\\', incorporating many elements of that film - particularly the actual footage. Bruce Lee\\'s character Billy Lo (apparently) investigates the sudden death of his friend and encounters a piece of film that was left with the man\\'s daughter. When the body is stolen during the funeral (!), Billy is also killed and it\\'s up to his wayward brother to avenge both men\\'s deaths.<br /><br />Tong Long stars as brother Bobby Lo and doesn\\'t really have the sort of charisma to carry the film. His fighting abilities are very good however. Bruce Lee obviously turns up thanks to (no longer) deleted footage simply to cash-in on the legacy. Saying that, on the whole, the footage is actually edited-in better than in \\'Game of Death\\' but it doesn\\'t stop the film from being a mess.<br /><br />OK, so the fights are actually very entertaining (dare I say mind-blowing) and make the film at least watchable. But there are so many daft elements to this film that it really tests your patience. First off, there\\'s the supposed villain who lives on his palatial estate... or is that mental institution? Seriously, the nutter eats raw venison, drinks deer\\'s blood, carries a monkey on his shoulder and owns some peacocks and lions (?!). This attempt to make him look tough and intelligent just makes you feel sorry for him - you half expect someone to escort him back to his room.<br /><br />In fact, this middle section is awful and when the scene involving a naked hooker and a lion suit arrived I turned it off. However, I did finish the film and was kind of glad I did because the fight scene towards the end (much like \\'GOD\\') was the whole reason for watching. While the story is an embarrassment, the action is very good and contains excellent choreography.<br /><br />But even the finale disappoints if the premise was anything to go by. What we were told was that the \\'Tower of Death\\' was a pagoda that was upside down and underground. This sounded great, like a twist on Bruce Lee\\'s original idea with different styles of fighting on each level. Could this be the \\'Game of Death\\' that was originally planned? No! The film should have been named \"Generator Room of Death\" because thats as far as the tower goes. Of yes, there were indeed one or two \\'different\\' styles... there were foil clad grunts, leopard-skinned henchman and stupid monk. It\\'s as though Enter the Dragon had never been made, with the plot being a poor imitation.<br /><br />Worth watching once for the fast paced fight scenes, but so stupid sometimes that it hurts. If this was intended, then fine. Thumbs up, however, for recreating that projector room scene from \\'Enter The Dragon\\'.'\n",
      "Label: 0\n",
      "Vectorized: [[  48   22   38  104  388  377    8 5988    5    1    5  343  301   12\n",
      "    24   62  575   45   29    4 2655   30  211    9  414   37   11   13\n",
      "   147    4  761    6 2784    1  237   70    4 1434  864    1   27 1160\n",
      "    10   19   43    1   31    6   83   15  507    5  343   45  231   29\n",
      "    50    4    1    5 2484    2 2706    1  104  773    5   12   19  555\n",
      "     2  760  906 1434 4652  108 1496 7689  658    1    2 1987  343    5\n",
      "    23  457    3 3619    4  400    5   19   12   13  311   15    2 1456\n",
      "   561   52    2  648    7 2726  293    2 4660 1496    7   78  544    3\n",
      "    29   56    6   23    1  588    6 8055  196 6746    1  202  389   14\n",
      "   588 3242 7689    3  144   62   25    2  413    5 3420    6 1633    2\n",
      "    19   23 1019 3921   22   51   49  198 1434  864  511  484   56 1218\n",
      "     6   55 1198 5565  906  318    6    1   20    2 5259  626   12   20\n",
      "     2  216    2  906    7  155    1  123   70    8  507    5  343   18\n",
      "     9  144  615    2   19   36  106    4    1   38    2 1806   22  155\n",
      "    51  441 3204   11  127    1    3   94    2   19   30  218 1777   18\n",
      "    48   22   38  104    1  773    6   10   19   12    9   62    1  122\n",
      "  5062   85  125  222    2  391 1031   35  443   20   23    1 3438   42\n",
      "     7   12 1648 5188  651    2    1 5274 2716    1 6456    1]]\n",
      "Vocabulary items:\n",
      "  1287 -> impression\n",
      "   313 -> american\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Build IMDB dataset\n",
    "dataset_path = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1\",\n",
    "    \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    untar=True,\n",
    "    cache_subdir=\"\",\n",
    ")\n",
    "\n",
    "dataset_directory = os.path.join(os.path.dirname(dataset_path), \"aclImdb\")\n",
    "\n",
    "shutil.rmtree(os.path.join(dataset_directory, \"train/unsup\"))\n",
    "\n",
    "print(f\"Dataset directory contents: {os.listdir(dataset_directory)}\")\n",
    "\n",
    "training_subdirectory_items = os.listdir(os.path.join(dataset_directory, \"train\"))\n",
    "print(f\"Training subdirectory contents: {training_subdirectory_items}\")\n",
    "\n",
    "print(\"Sample file contents:\")\n",
    "with open(os.path.join(dataset_directory, \"train/pos/1181_9.txt\")) as file:\n",
    "    print(file.read())\n",
    "\n",
    "seed = random.randint(0, 2**32 - 1)\n",
    "\n",
    "dataset_train_raw = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_directory, \"train\"),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "dataset_validate_raw = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_directory, \"train\"),\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "dataset_test_raw = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_directory, \"test\"), batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "# Create dataset standardizer\n",
    "def standardize(input):\n",
    "    return tf.strings.regex_replace(\n",
    "        tf.strings.regex_replace(tf.strings.lower(input), \"<br />\", \"\"),\n",
    "        f\"[{re.escape(string.punctuation)}]\",\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Create vectorization layer\n",
    "layer_vectorize = tf.keras.layers.TextVectorization(\n",
    "    standardize=standardize,\n",
    "    max_tokens=10000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=250,\n",
    ")\n",
    "\n",
    "# Adapter vectorization layer to corpus\n",
    "layer_vectorize.adapt(dataset_train_raw.map(lambda data, _: data))\n",
    "\n",
    "# Test out vectorization\n",
    "batch_text, batch_label = next(iter(dataset_train_raw))\n",
    "print(f\"Review: {batch_text[0]}\")\n",
    "print(f\"Label: {batch_label[0]}\")\n",
    "print(f\"Vectorized: {layer_vectorize(tf.expand_dims(batch_text[0], -1))}\")\n",
    "print(f\"Vocabulary items:\")\n",
    "print(f\"  1287 -> {layer_vectorize.get_vocabulary()[1287]}\")\n",
    "print(f\"   313 -> {layer_vectorize.get_vocabulary()[313]}\")\n",
    "print(f\"Vocabulary size: {len(layer_vectorize.get_vocabulary())}\")\n",
    "\n",
    "# Apply vectorization to datasets\n",
    "dataset_train = (\n",
    "    dataset_train_raw.map(\n",
    "        lambda data, label: (layer_vectorize(tf.expand_dims(data, -1)), label)\n",
    "    )\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset_validate = (\n",
    "    dataset_validate_raw.map(\n",
    "        lambda data, label: (layer_vectorize(tf.expand_dims(data, -1)), label)\n",
    "    )\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset_test = (\n",
    "    dataset_test_raw.map(\n",
    "        lambda data, label: (layer_vectorize(tf.expand_dims(data, -1)), label)\n",
    "    )\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(10001, 16),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 12s 17ms/step - loss: 0.6659 - binary_accuracy: 0.6948 - val_loss: 0.6160 - val_binary_accuracy: 0.7684\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.5513 - binary_accuracy: 0.8011 - val_loss: 0.4994 - val_binary_accuracy: 0.8190\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.4478 - binary_accuracy: 0.8431 - val_loss: 0.4231 - val_binary_accuracy: 0.8450\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 5s 8ms/step - loss: 0.3818 - binary_accuracy: 0.8626 - val_loss: 0.3783 - val_binary_accuracy: 0.8552\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.3394 - binary_accuracy: 0.8780 - val_loss: 0.3508 - val_binary_accuracy: 0.8600\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.3094 - binary_accuracy: 0.8871 - val_loss: 0.3328 - val_binary_accuracy: 0.8642\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2854 - binary_accuracy: 0.8957 - val_loss: 0.3205 - val_binary_accuracy: 0.8672\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.2659 - binary_accuracy: 0.9030 - val_loss: 0.3118 - val_binary_accuracy: 0.8698\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2486 - binary_accuracy: 0.9089 - val_loss: 0.3059 - val_binary_accuracy: 0.8708\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.2350 - binary_accuracy: 0.9143 - val_loss: 0.3024 - val_binary_accuracy: 0.8744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b00f960f40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset_train, validation_data=dataset_validate, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 23s 29ms/step - loss: 0.3156 - binary_accuracy: 0.8706\n",
      "Loss: 0.3156213164329529, accuracy: 0.8705999851226807\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(dataset_test)\n",
    "\n",
    "print(f\"Loss: {loss}, accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
